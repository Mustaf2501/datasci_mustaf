{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipes for data import and cleanup\n",
    "\n",
    "In my experience, around 75% of the time you spend working with data will be fighting to import it and clean it up. For the most part this is just general-purpose programming, but there are a few library routines that will save you from reinventing the wheel.\n",
    "\n",
    "* [Preamble](#preamble)\n",
    "* [Reading from a csv file](#readfile)\n",
    "* [Reading from a string](#readstr)\n",
    "* [Reading from a url or string](#readurl)\n",
    "* [Parsing a log file with regular expressions](#regexp)\n",
    "* [Reading JSON from a web service](#json)\n",
    "* [Scraping a website with xpath](#xpath)\n",
    "* [Querying an SQL database](#sql)\n",
    "\n",
    "Treat this section as a collection of recipes and pointers to useful library routines. If you find yourself needing them, you should read the recipe, try it out, then look online for more information about the library functions it suggests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from a csv file <span id=\"readfile\"></span>\n",
    "\n",
    "When our data is a very simple comma-separated value (CSV) file then it's very easy to import with `pandas.read_csv`. We can specify either a filename or a url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv('data/iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our file is nearly a CSV but has some quirks such as comments or a missing header row, there are plenty of options in [`pandas.read_csv`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) or [`pandas.read_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html). For extreme quirks we may need to use the raw Python [`csv.reader`](https://docs.python.org/3/library/csv.html#csv.reader)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from a string<span id=\"readstr\"></span>\n",
    "\n",
    "If we have a string that we want to treat as a file, we can use `io.StringIO`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "data = '''\n",
    "x,y\n",
    "10,3\n",
    "2,5\n",
    "'''\n",
    "\n",
    "df = pandas.read_csv(io.StringIO(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from an http request<span id=\"readurl\"></span>\n",
    "\n",
    "If we want to read from a url but we want more control over the http request, for example sending a POST request or modifying the request header or reading the response header, we can use the `requests` library to fetch the data as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "\n",
    "my_url = \"https://www.cl.cam.ac.uk/teaching/current/DataSci/data/iris.csv\"\n",
    "data = requests.get(my_url).content.decode('utf8')\n",
    "df = pandas.read_csv(io.StringIO(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing log files with regular expressions<span id=\"regexp\"></span>\n",
    "A typical line from a web server log might look like this\n",
    "```\n",
    "207.46.13.169 - - [27/Aug/2017:06:52:11 +0000] \"GET /marcus/essay/st&h2.html HTTP/1.1\" 200 3881 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 7_0 like Mac OS X) AppleWebKit/537.51.1 (KHTML, like Gecko) Version/7.0 Mobile/11A465 Safari/9537.53 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)\"\n",
    "```\n",
    "where (according to the [Apache web server documentation](https://httpd.apache.org/docs/2.4/logs.html#combined)) the pieces are\n",
    "\n",
    "* **`207.46.13.169`** \n",
    "The IP address that made the request\n",
    "* **`-`** \n",
    "The identity of the client; `-` means not available\n",
    "* **`-`**\n",
    "The userid of the logged-in user who made the request; `-` means not available\n",
    "* **`[27/Aug/2017:06:52:11 +0000]`**\n",
    "The time the request was received\n",
    "* **`\"GET /marcus/essay/st&h2.html HTTP/1.1\"`**\n",
    "The type of request, what was requested, and the connection type\n",
    "* **`200`**\n",
    "The [http status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) (200 means OK)\n",
    "* **`3881`**\n",
    "The size of the object returned to the client, in bytes\n",
    "* **`-`**\n",
    "The referrer URL; `-` means not available\n",
    "* **`\"Mozilla/5.0 (...)\"`**\n",
    "The browser type. The substring `bingbot` here tells us that the request comes from Microsoft Bing's web crawler.\n",
    "\n",
    "To extract these pieces from a single line of the log file, the best tool is [regular expressions](https://docs.python.org/3.4/library/re.html), a mini-language for string matching that is common across many programming languages. The syntax is terse and takes a lot of practice. I like to start with a small string pattern and incrementally build it up, testing as I go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 16), match='\\n207.46.13.169 -'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re    # standard Python module for regular expressions\n",
    "\n",
    "s = \"\"\"\n",
    "207.46.13.169 - - [27/Aug/2017:06:52:11 +0000] \"GET /marcus/essay/st&h2.html HTTP/1.1\" \n",
    "200 3881 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 7_0 like Mac OS X)\n",
    "AppleWebKit/537.51.1 (KHTML, like Gecko) Version/7.0 Mobile/11A465 Safari/9537.53\n",
    "(compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)\"\n",
    "\"\"\"\n",
    "\n",
    "# First attempt: match the first two items in the log line.\n",
    "# If my pattern is right, re.match returns an object.\n",
    "# If my pattern is wrong, re.match returns None.\n",
    "pattern_test = r'\\s*(\\S+)\\s*(\\S+)'\n",
    "re.match(pattern_test, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ip': '207.46.13.169',\n",
       " 'client': '-',\n",
       " 'user': '-',\n",
       " 't': '27/Aug/2017:06:52:11 +0000',\n",
       " 'req': 'GET /marcus/essay/st&h2.html HTTP/1.1',\n",
       " 'status': '200',\n",
       " 'size': '3881',\n",
       " 'ref': '-',\n",
       " 'ua': 'Mozilla/5.0 (iPhone; CPU iPhone OS 7_0 like Mac OS X)\\nAppleWebKit/537.51.1 (KHTML, like Gecko) Version/7.0 Mobile/11A465 Safari/9537.53\\n(compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the full pattern I built up to. Python lets us add verbose comments\n",
    "# to the pattern, which is handy for remembering what your code does when you look\n",
    "# at it the next morning.\n",
    "pattern = r'''(?x)  #   flag saying that this pattern has comments\n",
    "\\s*                 #   any whitespace at the start of the string\n",
    "(?P<ip>\\S+)         # one or more non-space characters: the IP address\n",
    "\\s+                 #   one or more spaces\n",
    "(?P<client>\\S+)     # the client identity\n",
    "\\s+\n",
    "(?P<user>\\S+)       # the userid\n",
    "\\s+\n",
    "\\[(?P<t>[^\\]]*)\\]   # [, then any number of not-] characters, then ]: the timestamp\n",
    "\\s+\n",
    "\"(?P<req>[^\"]*)\"    # \", then any number of not-\" characters, then \": the request string\n",
    "\\s+\n",
    "(?P<status>\\d+)     # one or more numerical digits: the http status code\n",
    "\\s+\n",
    "(?P<size>\\d+)       # one or more numerical digits: the size\n",
    "\\s+\n",
    "\"(?P<ref>[^\"]*)\"    # the referrer URL\n",
    "\\s+\n",
    "\"(?P<ua>[^\"]*)\"     # the user agent i.e. browser type\n",
    "'''\n",
    "m = re.match(pattern, s)\n",
    "m.groupdict()       # returns a dictionary of all the named sub-patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we extract the fields from a full log file? The vanilla Python code is\n",
    "```\n",
    "with open(myfile) as f:\n",
    "    for line in f:\n",
    "        m = re.match(pattern, line)\n",
    "        # store the fields from m.groups() or m.groupdict() somewhere appropriate\n",
    "```\n",
    "Alternatively, `numpy` has a handy shortcut for reading in an entire file and splitting it via a regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>client</th>\n",
       "      <th>user</th>\n",
       "      <th>t</th>\n",
       "      <th>req</th>\n",
       "      <th>status</th>\n",
       "      <th>size</th>\n",
       "      <th>ref</th>\n",
       "      <th>ua</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>207.46.13.85</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>27/Aug/2017:08:26:01 +0000</td>\n",
       "      <td>GET /damon/recipe/breadbutterpud2 HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>1635</td>\n",
       "      <td>-</td>\n",
       "      <td>Mozilla/5.0 (iPhone; CPU iPhone OS 7_0 like Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>216.244.66.250</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>27/Aug/2017:08:02:49 +0000</td>\n",
       "      <td>GET /robots.txt HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>359</td>\n",
       "      <td>-</td>\n",
       "      <td>Mozilla/5.0 (compatible; DotBot/1.1; http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>86.183.91.158</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>27/Aug/2017:07:50:23 +0000</td>\n",
       "      <td>GET /favicon.ico HTTP/1.1</td>\n",
       "      <td>404</td>\n",
       "      <td>503</td>\n",
       "      <td>-</td>\n",
       "      <td>MobileSafari/602.1 CFNetwork/811.5.4 Darwin/16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>73.246.74.104</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>27/Aug/2017:08:08:02 +0000</td>\n",
       "      <td>GET /irene/cross/12-jesus-dies-on-the-cross.jp...</td>\n",
       "      <td>200</td>\n",
       "      <td>77292</td>\n",
       "      <td>https://www.bing.com/</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 10.0; Win64; x64) Appl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ip client user                           t  \\\n",
       "362    207.46.13.85      -    -  27/Aug/2017:08:26:01 +0000   \n",
       "224  216.244.66.250      -    -  27/Aug/2017:08:02:49 +0000   \n",
       "142   86.183.91.158      -    -  27/Aug/2017:07:50:23 +0000   \n",
       "243   73.246.74.104      -    -  27/Aug/2017:08:08:02 +0000   \n",
       "\n",
       "                                                   req  status   size  \\\n",
       "362         GET /damon/recipe/breadbutterpud2 HTTP/1.1     200   1635   \n",
       "224                           GET /robots.txt HTTP/1.1     200    359   \n",
       "142                          GET /favicon.ico HTTP/1.1     404    503   \n",
       "243  GET /irene/cross/12-jesus-dies-on-the-cross.jp...     200  77292   \n",
       "\n",
       "                       ref                                                 ua  \n",
       "362                      -  Mozilla/5.0 (iPhone; CPU iPhone OS 7_0 like Ma...  \n",
       "224                      -  Mozilla/5.0 (compatible; DotBot/1.1; http://ww...  \n",
       "142                      -  MobileSafari/602.1 CFNetwork/811.5.4 Darwin/16...  \n",
       "243  https://www.bing.com/  Mozilla/5.0 (Windows NT 10.0; Win64; x64) Appl...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the file into an array, one row per line, one column per field in the pattern\n",
    "t = [('ip',str,30), ('client',str,10), ('userid',str,10), ('timestamp',str,80), \n",
    "     ('req',str,300), ('status',int), ('size',int), \n",
    "     ('ref',str,300), ('uagent',str,300)]\n",
    "df = np.fromregex('data/webaccess_short.log', pattern, dtype=t)\n",
    "\n",
    "# Make a dictionary out of the columns, according to the named fields in the pattern\n",
    "df = pandas.DataFrame({v: df[n] for (v,_),n in zip(re.compile(pattern).groupindex.items(), df.dtype.names)})\n",
    "\n",
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading json from a web data service<span id=\"json\"></span>\n",
    "More and more forward-thinking companies and government services make data available by simple web requests. Here is an example, stop-and-search data from [data.police.uk](https://data.police.uk/).\n",
    "\n",
    "The first step is to import the Python module for making web requests. When I'm developing data code I like to build it up in small steps, which means lots of repeated requests, so I also like to use another Python module which caches responses. This means I don't hammer the service unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, requests_cache\n",
    "requests_cache.install_cache('data/stopsearch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [API documentation](https://data.police.uk/docs/) tells us the URL for fetching a list of available data. Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the request. This should print out <Response [200]>, meaning successfully retrieved\n",
    "AVAILABILITY_URL = 'https://data.police.uk/api/crimes-street-dates'\n",
    "stations_resp = requests.get(AVAILABILITY_URL)\n",
    "stations_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the body of the response. It's likely to be very long, so we'll only print out the first 300 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"date\":\"2023-09\",\"stop-and-search\":[\"avon-and-somerset\",\"bedfordshire\",\"btp\",\"cambridgeshire\",\"city-of-london\",\"cleveland\",\"cumbria\",\"derbyshire\",\"dorset\",\"durham\",\"essex\",\"gloucestershire\",\"hampshire\",\"hertfordshire\",\"kent\",\"lancashire\",\"leicestershire\",\"lincolnshire\",\"merseyside\",\"norfolk\",\"nort'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_resp.text[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like [JSON](https://en.wikipedia.org/wiki/JSON), \"JavaScript Object Notation\", a common format for web data services. \n",
    "\n",
    "It's easy to convert it into Python dictionaries and lists, with `requests.get(...).json()`. Now we can explore what it contains. (Alternatively, just read the web service documentation, if we trust it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'dict'>\n",
      "dict_keys(['date', 'stop-and-search'])\n",
      "{'date': '2023-09', 'stop-and-search': ['avon-and-somerset', 'bedfordshire', 'btp', 'cambridgeshire', 'city-of-london', 'cleveland', 'cumbria', 'derbyshire', 'dorset', 'durham', 'essex', 'gloucestershire', 'hampshire', 'hertfordshire', 'kent', 'lancashire', 'leicestershire', 'lincolnshire', 'merseyside', 'norfolk', 'north-wales', 'north-yorkshire', 'northamptonshire', 'northumbria', 'south-yorkshire', 'staffordshire', 'suffolk', 'surrey', 'sussex', 'thames-valley', 'west-mercia', 'west-midlands', 'west-yorkshire']}\n"
     ]
    }
   ],
   "source": [
    "x = requests.get(AVAILABILITY_URL).json()\n",
    "\n",
    "print(type(x))      # x = [...]\n",
    "print(type(x[0]))   # x = [{...}, ...]\n",
    "print(x[0].keys())  # x = [{date, stop-and-search}, ...]\n",
    "print(x[0])         # x = [{date, stop-and-search:[area,area,...]}, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has a pretty flexible command for converting nested JSON into nice sane dataframes, [`pandas.json_normalize`](https://pandas.pydata.org/pandas-docs/version/1.2.0/reference/api/pandas.json_normalize.html#pandas.json_normalize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>force</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>avon-and-somerset</td>\n",
       "      <td>2023-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bedfordshire</td>\n",
       "      <td>2023-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>btp</td>\n",
       "      <td>2023-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cambridgeshire</td>\n",
       "      <td>2023-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>city-of-london</td>\n",
       "      <td>2023-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>warwickshire</td>\n",
       "      <td>2020-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>west-mercia</td>\n",
       "      <td>2020-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>west-midlands</td>\n",
       "      <td>2020-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>west-yorkshire</td>\n",
       "      <td>2020-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>wiltshire</td>\n",
       "      <td>2020-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1416 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  force     date\n",
       "0     avon-and-somerset  2023-09\n",
       "1          bedfordshire  2023-09\n",
       "2                   btp  2023-09\n",
       "3        cambridgeshire  2023-09\n",
       "4        city-of-london  2023-09\n",
       "...                 ...      ...\n",
       "1411       warwickshire  2020-10\n",
       "1412        west-mercia  2020-10\n",
       "1413      west-midlands  2020-10\n",
       "1414     west-yorkshire  2020-10\n",
       "1415          wiltshire  2020-10\n",
       "\n",
       "[1416 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "availability = pandas.json_normalize(x, 'stop-and-search', 'date').rename(columns={0:'force'})\n",
    "availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [police API documentation](https://data.police.uk/docs/method/stops-force/) tells us how to request records for a given police force and month: using a url of the form\n",
    "```\n",
    "https://data.police.uk/api/stops-force?force=avon-and-somerset&date=2017-01\n",
    "```\n",
    "Let's fetch them all. For each item we'll fetch the data, turn it into a dataframe (again using `pandas.json_normalize`), and then we'll bind everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://data.police.uk/api/stops-force?force=durham&date=2023-09 ... ... ......\r"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "BASE_URL = 'https://data.police.uk/api/'\n",
    "STOPSDATA_URL = urllib.parse.urljoin(BASE_URL, 'stops-force')\n",
    "availability['url'] = [STOPSDATA_URL + '?' + urllib.parse.urlencode({'force':f, 'date':d}) \n",
    "                       for f,d in zip(availability.force, availability.date)]\n",
    "\n",
    "def get_dataframe(q):\n",
    "    url,date,force = (q.url, q.date, q.force)\n",
    "    print(url + \" ...\", end=\"\\r\")\n",
    "    response = requests.get(url)\n",
    "    x = response.json()\n",
    "    df = pandas.json_normalize(x, sep='_')\n",
    "    df.insert(0, 'month', date)\n",
    "    df.insert(0, 'force', force)\n",
    "    return df\n",
    "\n",
    "# only fetch a little, for illustration purposes\n",
    "df = [get_dataframe(r) for i,r in availability.iloc[:10].iterrows()]\n",
    "stopsearch = pandas.concat(df, axis=0, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>force</th>\n",
       "      <th>month</th>\n",
       "      <th>age_range</th>\n",
       "      <th>outcome</th>\n",
       "      <th>involved_person</th>\n",
       "      <th>self_defined_ethnicity</th>\n",
       "      <th>gender</th>\n",
       "      <th>legislation</th>\n",
       "      <th>outcome_linked_to_object_of_search</th>\n",
       "      <th>datetime</th>\n",
       "      <th>...</th>\n",
       "      <th>officer_defined_ethnicity</th>\n",
       "      <th>type</th>\n",
       "      <th>operation_name</th>\n",
       "      <th>object_of_search</th>\n",
       "      <th>outcome_object_id</th>\n",
       "      <th>outcome_object_name</th>\n",
       "      <th>location_latitude</th>\n",
       "      <th>location_street_id</th>\n",
       "      <th>location_street_name</th>\n",
       "      <th>location_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>bedfordshire</td>\n",
       "      <td>2023-09</td>\n",
       "      <td>18-24</td>\n",
       "      <td>A no further action disposal</td>\n",
       "      <td>True</td>\n",
       "      <td>White - English/Welsh/Scottish/Northern Irish/...</td>\n",
       "      <td>Male</td>\n",
       "      <td>Misuse of Drugs Act 1971 (section 23)</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-09-15T22:03:42+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>White</td>\n",
       "      <td>Person search</td>\n",
       "      <td>None</td>\n",
       "      <td>Controlled drugs</td>\n",
       "      <td>bu-no-further-action</td>\n",
       "      <td>A no further action disposal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>avon-and-somerset</td>\n",
       "      <td>2023-09</td>\n",
       "      <td>over 34</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>Other ethnic group - Not stated</td>\n",
       "      <td>Female</td>\n",
       "      <td>Police and Criminal Evidence Act 1984 (section 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-09-25T00:00:00+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>White</td>\n",
       "      <td>Person search</td>\n",
       "      <td>None</td>\n",
       "      <td>Article for use in theft</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>51.456677</td>\n",
       "      <td>2278864.0</td>\n",
       "      <td>On or near Nightclub</td>\n",
       "      <td>-2.592162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3959</th>\n",
       "      <td>durham</td>\n",
       "      <td>2023-09</td>\n",
       "      <td>25-34</td>\n",
       "      <td>A no further action disposal</td>\n",
       "      <td>True</td>\n",
       "      <td>White - English/Welsh/Scottish/Northern Irish/...</td>\n",
       "      <td>Male</td>\n",
       "      <td>Misuse of Drugs Act 1971 (section 23)</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-09-03T02:00:00+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>White</td>\n",
       "      <td>Person search</td>\n",
       "      <td>None</td>\n",
       "      <td>Controlled drugs</td>\n",
       "      <td>bu-no-further-action</td>\n",
       "      <td>A no further action disposal</td>\n",
       "      <td>54.727183</td>\n",
       "      <td>2166006.0</td>\n",
       "      <td>On or near Parking Area</td>\n",
       "      <td>-1.252132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  force    month age_range                       outcome  \\\n",
       "720        bedfordshire  2023-09     18-24  A no further action disposal   \n",
       "332   avon-and-somerset  2023-09   over 34                                 \n",
       "3959             durham  2023-09     25-34  A no further action disposal   \n",
       "\n",
       "      involved_person                             self_defined_ethnicity  \\\n",
       "720              True  White - English/Welsh/Scottish/Northern Irish/...   \n",
       "332              True                    Other ethnic group - Not stated   \n",
       "3959             True  White - English/Welsh/Scottish/Northern Irish/...   \n",
       "\n",
       "      gender                                        legislation  \\\n",
       "720     Male              Misuse of Drugs Act 1971 (section 23)   \n",
       "332   Female  Police and Criminal Evidence Act 1984 (section 1)   \n",
       "3959    Male              Misuse of Drugs Act 1971 (section 23)   \n",
       "\n",
       "     outcome_linked_to_object_of_search                   datetime  ...  \\\n",
       "720                                None  2023-09-15T22:03:42+00:00  ...   \n",
       "332                                None  2023-09-25T00:00:00+00:00  ...   \n",
       "3959                              False  2023-09-03T02:00:00+00:00  ...   \n",
       "\n",
       "     officer_defined_ethnicity           type operation_name  \\\n",
       "720                      White  Person search           None   \n",
       "332                      White  Person search           None   \n",
       "3959                     White  Person search           None   \n",
       "\n",
       "              object_of_search     outcome_object_id  \\\n",
       "720           Controlled drugs  bu-no-further-action   \n",
       "332   Article for use in theft                         \n",
       "3959          Controlled drugs  bu-no-further-action   \n",
       "\n",
       "               outcome_object_name location_latitude location_street_id  \\\n",
       "720   A no further action disposal               NaN                NaN   \n",
       "332                                        51.456677          2278864.0   \n",
       "3959  A no further action disposal         54.727183          2166006.0   \n",
       "\n",
       "         location_street_name location_longitude  \n",
       "720                       NaN                NaN  \n",
       "332      On or near Nightclub          -2.592162  \n",
       "3959  On or near Parking Area          -1.252132  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopsearch.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a website with xpath<span id=\"xpath\"></span>\n",
    "\n",
    "There are fascinating stories to be discovered from public data, and sometimes you have to work to scrape it from web pages. Here's [an acount](https://onlinejournalismblog.com/2016/11/29/how-the-bbc-england-data-unit-scraped-airport-noise-complaints/) by a BBC data journalist. We'll work with a very simple example: extracting results of the Oxford / Cambridge boat race from the [Wikipedia table](https://en.wikipedia.org/wiki/List_of_The_Boat_Race_results#Main_race).\n",
    "\n",
    "I recommend using [XPath queries](https://www.w3.org/TR/xpath20/) from the [`lxml`](http://lxml.de/) module.\n",
    "XPath is a powerful mini-language for extracting data from hierarchical documents, with wide support across many programming languages &mdash; think of it as regular expressions but for html rather than plain text. If you want to scrape websites then it's worth finding a tutorial and learning XPath. For this course, we'll just see how to use XPath in Python.\n",
    "\n",
    "The first step is to install `lxml`, which is not included with Python.\n",
    "```\n",
    "!pip install lxml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fetch the web page and parse the contents. Most web pages are badly-formatted html (sections not properly closed, etc.), and `lxml.html.fromstring` makes a reasonable attempt to make sense of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml.html\n",
    "\n",
    "BOATRACE_URL = 'https://en.wikipedia.org/wiki/List_of_The_Boat_Race_results'\n",
    "resp = requests.get(BOATRACE_URL)\n",
    "doc = lxml.html.fromstring(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us `doc`, the root `<html>` element, which we can inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html\n",
      "2\n",
      "['head', 'body']\n",
      "{'class': 'client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-zebra-design-disabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-typography-survey-disabled vector-toc-available', 'lang': 'en', 'dir': 'ltr'}\n",
      "\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "print(doc.tag)               # the type of element\n",
    "print(len(doc))              # the number of children\n",
    "print([n.tag for n in doc])  # tags of its children, <head> and <body>\n",
    "print(doc.attrib)            # get the attributes, e.g. <html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
    "print(doc.text, doc.tail)    # any text directly under in this element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to pull out a particular element from the document, namely the table with boat race results, so we need to work out how to refer to it in XPath. The Chrome webbrowser has a handy tool to help with this. Go to the page you're interested in, and click on &hellip; | More tools | Developer Tools. Click on the element-selector button at the top left:\n",
    "<img src=\"res/xpath1.png\" alt=\"use 'select element' mode\">\n",
    "Go back to the web page, and click on a piece close to what you want to select. I clicked on the top left cell of the table:\n",
    "<img src=\"res/xpath2.png\" alt=\"click roughly where you want\" style=\"height:8em\">\n",
    "Go back to the developer tools window, and navigate to the exact element you want. Here, we want the table. Right-click and choose Copy | Copy XPath. \n",
    "<img src=\"res/xpath3.png\" alt=\"copy the XPath of the element you want\" style=\"height:8em\">\n",
    "It gave me the XPath location `\"//*[@id=\"mw-content-text\"]/div[1]/table[2]\"`. Now we can extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tr>\n",
      "<td><a href=\"/wiki/The_Boat_Race_1841\" title=\"The Boat Race 1841\">5</a></td>\n",
      "<td><span data-sort-value=\"000000001841-04-14-0000\" style=\"white-space:nowrap\">14 April 1841</span> ‡</td>\n",
      "<td style=\"background:#B7E1E4; color:#000;\">Cambridge</td>\n",
      "<td>32:03</td>\n",
      "<td>22 lengths</td>\n",
      "<td>1</td>\n",
      "<td>4\n",
      "</td></tr>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 June 1829</td>\n",
       "      <td>Oxford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17 June 1836</td>\n",
       "      <td>Cambridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 April 1839</td>\n",
       "      <td>Cambridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15 April 1840</td>\n",
       "      <td>Cambridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14 April 1841</td>\n",
       "      <td>Cambridge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               t     winner\n",
       "0   10 June 1829     Oxford\n",
       "1   17 June 1836  Cambridge\n",
       "2   3 April 1839  Cambridge\n",
       "3  15 April 1840  Cambridge\n",
       "4  14 April 1841  Cambridge"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick out the table element.\n",
    "# (XPath queries return lists, but I only want one item, hence the [0].)\n",
    "table = doc.xpath('//*[@id=\"mw-content-text\"]/div[1]/table[2]')[0]\n",
    "\n",
    "# Get a list of all rows i.e. <tr> elements inside the table.\n",
    "# Print one, to check things look OK.\n",
    "rows = table.xpath('.//tr')\n",
    "print(lxml.etree.tostring(rows[5], encoding='unicode'))\n",
    "\n",
    "# Extract the timestamp and winner columns.\n",
    "# The timestamp is in the second child, in a <span> element with a \"data-sort-value\" attribute.\n",
    "# The winner is in the third child.\n",
    "df = {'t': [row[1].xpath('.//span[@data-sort-value]')[0].text for row in rows[1:]],\n",
    "      'winner': [row[2].text for row in rows[1:]]}\n",
    "df = pandas.DataFrame(df)\n",
    "df.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should consider the ethics of your web scraping. Here are some thoughts:\n",
    "from [Sophie Chou at the MIT Media Lab](http://www.storybench.org/to-scrape-or-not-to-scrape-the-technical-and-ethical-challenges-of-collecting-data-off-the-web/), and the [data journalist N&auml;el Shiab](https://gijn.org/2015/08/12/on-the-ethics-of-web-scraping-and-data-journalism/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying an SQL database<span id=\"sql\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your data is in an SQL database, access is easy. Here's an example with Postgresql, a dialect of SQL.\n",
    "\n",
    "Databases are usually secured, and you need various credentials to log in. \n",
    "It's good practice to store our code in a repository, but bad practice to store credentials\n",
    "there too. Instead, we can store credentials in separate file that isn't checked in to the repository.\n",
    "I like to store credentials in a JSON file, something like this:\n",
    "```\n",
    "{\"user\": \"SPQR\", \"password\": \"TOPSECRET\", \"host\": \"***\", \"dbname\": \"***\"}\n",
    "```\n",
    "which is easy to load into Python as a dictionary. Then I can use the fields of this dictionary as arguments to `psycopg2.connect`, to establish the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2    # module for connecting to a Postgresql database\n",
    "import json        # standard module for reading json files\n",
    "\n",
    "creds = json.load(open('res/secret_creds.json'))\n",
    "conn = psycopg2.connect(**creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run arbitrary SQL queries, and retrieve the results as a pandas dataframe. Pass in parameters with the `%(name)s` quoting mechanism, to keep ourselves safe from SQL injection attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uri</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>catchment</th>\n",
       "      <th>river</th>\n",
       "      <th>town</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>index</th>\n",
       "      <th>measure_id</th>\n",
       "      <th>uri</th>\n",
       "      <th>station_uri</th>\n",
       "      <th>qualifier</th>\n",
       "      <th>parameter</th>\n",
       "      <th>period</th>\n",
       "      <th>unit</th>\n",
       "      <th>valuetype</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>345</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Great Chesterford</td>\n",
       "      <td>E21778</td>\n",
       "      <td>Cam and Ely Ouse (Including South Level)</td>\n",
       "      <td>River Cam</td>\n",
       "      <td>Great Chesterford</td>\n",
       "      <td>52.061730</td>\n",
       "      <td>0.194279</td>\n",
       "      <td>397</td>\n",
       "      <td>398</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Stage</td>\n",
       "      <td>Water Level</td>\n",
       "      <td>900.0</td>\n",
       "      <td>m</td>\n",
       "      <td>instantaneous</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>800</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Weston Bampfylde</td>\n",
       "      <td>52113</td>\n",
       "      <td>Parrett, Brue and West Somerset Streams</td>\n",
       "      <td>River Cam</td>\n",
       "      <td>Weston Bampfylde</td>\n",
       "      <td>51.023159</td>\n",
       "      <td>-2.565568</td>\n",
       "      <td>918</td>\n",
       "      <td>919</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Stage</td>\n",
       "      <td>Water Level</td>\n",
       "      <td>900.0</td>\n",
       "      <td>m</td>\n",
       "      <td>instantaneous</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1272</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Cambridge Baits Bite</td>\n",
       "      <td>E60101</td>\n",
       "      <td>Cam and Ely Ouse (Including South Level)</td>\n",
       "      <td>River Cam</td>\n",
       "      <td>Milton</td>\n",
       "      <td>52.236542</td>\n",
       "      <td>0.176925</td>\n",
       "      <td>1454</td>\n",
       "      <td>1455</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Stage</td>\n",
       "      <td>Water Level</td>\n",
       "      <td>900.0</td>\n",
       "      <td>mASD</td>\n",
       "      <td>instantaneous</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                                uri  \\\n",
       "0    345  http://environment.data.gov.uk/flood-monitorin...   \n",
       "1    800  http://environment.data.gov.uk/flood-monitorin...   \n",
       "2   1272  http://environment.data.gov.uk/flood-monitorin...   \n",
       "\n",
       "                  label      id                                 catchment  \\\n",
       "0     Great Chesterford  E21778  Cam and Ely Ouse (Including South Level)   \n",
       "1      Weston Bampfylde   52113   Parrett, Brue and West Somerset Streams   \n",
       "2  Cambridge Baits Bite  E60101  Cam and Ely Ouse (Including South Level)   \n",
       "\n",
       "       river               town        lat       lng  index  measure_id  \\\n",
       "0  River Cam  Great Chesterford  52.061730  0.194279    397         398   \n",
       "1  River Cam   Weston Bampfylde  51.023159 -2.565568    918         919   \n",
       "2  River Cam             Milton  52.236542  0.176925   1454        1455   \n",
       "\n",
       "                                                 uri  \\\n",
       "0  http://environment.data.gov.uk/flood-monitorin...   \n",
       "1  http://environment.data.gov.uk/flood-monitorin...   \n",
       "2  http://environment.data.gov.uk/flood-monitorin...   \n",
       "\n",
       "                                         station_uri qualifier    parameter  \\\n",
       "0  http://environment.data.gov.uk/flood-monitorin...     Stage  Water Level   \n",
       "1  http://environment.data.gov.uk/flood-monitorin...     Stage  Water Level   \n",
       "2  http://environment.data.gov.uk/flood-monitorin...     Stage  Water Level   \n",
       "\n",
       "   period  unit      valuetype    low   high  \n",
       "0   900.0     m  instantaneous  0.109  0.333  \n",
       "1   900.0     m  instantaneous  0.026  0.600  \n",
       "2   900.0  mASD  instantaneous  0.218  0.294  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = '''\n",
    "SELECT *\n",
    "FROM flood_stations AS s JOIN flood_measures AS m ON (m.station_uri = s.uri) \n",
    "WHERE river = %(river)s OR town = %(town)s\n",
    "LIMIT 3\n",
    "'''\n",
    "pandas.read_sql(cmd, conn, params={'river': 'River Cam', 'town': 'Cambridge'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
